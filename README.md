# LIVE DEPLOYMENT NOT POSSIBLE ON RENDER DUE TO BIG MODEL SIZE CAUSING GIT TO CHOKE ON PUSHES. LIVE DOWNLOAD LINK ON itch.io NOT AVAILABLE EITHER DUE TO HUGE SIZE OF MODEL.


# DOWNLOAD THE EXECUTEABLE .exe FILE (THESE ARE OLD VERSIONS WITHOUT THE EXTERNAL VALIDATION MODULES)

FROM DRIVE (https://drive.google.com/drive/folders/1uQYrXVQ6V-DJXEkGeUqBnXd-AaHEbwOa?usp=drive_link).

FROM file.kiwi (https://file.kiwi/d11ef2e1#EaDDD0zIu0L6KLetXWB9lw).

# ğŸ“° Fake News Detection System

An end-to-end **Fake News Detection** web application that evaluates online news articles using **Natural Language Processing**, **source credibility analysis**, and **external content validation** to estimate the likelihood of misinformation.

<img src="images/icon.ico" height="400" width="200">

The system is designed to be **modular**, **explainable**, and **privacy-conscious**, combining machine learning inference with transparent multi-factor scoring and a clean, accessible web interface.

<img src="images/output1.png" height="700" width="850">

---

## âœ¨ Key Features

* ğŸ” **Content Analysis with BERT**
  Classifies articles into **Neutral**, **Biased**, or **Contradictory** using a fine-tuned transformer-based language model.

* ğŸ›ï¸ **Publisher Credibility Scoring**
  Evaluates the historical reliability of known publishers using a curated Supabase database.

* ğŸŒ **External Content Validation**
  Scans the broader web to check whether similar reporting exists across multiple independent sources, strengthening confidence in widely corroborated news.

* ğŸ§® **Explainable Scoring Framework (Out of 30)**
  Combines content quality, publisher reliability, and external corroboration into a transparent, interpretable score.

* ğŸŒ **Flexible Input Methods**
  Supports both **article URLs** (auto-scraped) and **raw article text** (direct input).

- ğŸ” **Privacy First Architecture**
  No article content or user data is stored. All processing is performed in-memory and discarded after analysis.

---

## ğŸ§  How the System Works

<img src="images/New DFD.png" height="2000" width="4000">

1. **User Input**
   The user submits either a news article link or raw article text.

2. **Article Extraction**

   * URLs are scraped using **Firecrawl**, with a BeautifulSoup-based fallback.
   * Raw text is passed directly into the analysis pipeline.

3. **BERT-Based Content Classification**
   The article is classified into one of three categories:

   * Neutral
   * Biased
   * Contradictory

4. **Multi-Factor Scoring Logic (0â€“30)**

   **Category Score (0â€“10)**

   * Neutral â†’ 10
   * Biased â†’ 5
   * Contradictory â†’ 0

   **Publisher Credibility Score (0â€“10)**

   * Retrieved from Supabase for known publishers
   * Defaults to 0 if the publisher is unknown

   **External Validation Score (0â€“10)**

   * Measures whether similar or matching news appears across other reputable sources
   * Higher scores indicate stronger cross-source corroboration

5. **Final Verdict Generation**

   ```text
   Total Score = Category Score + Publisher Score + External Validation Score

   â‰¥ 20  â†’ Possible high credibility
   7â€“19  â†’ Needs further verification
   < 7   â†’ Credibility uncertain
   ```

The final verdict is designed to assist decision-making while clearly exposing how each component contributes to the result.

---

## ğŸ–¥ï¸ Web Interface

### Home Page

<img src="images/ui1.png" height="2000" width="4000">

Users can choose whether to submit an **article link** or **article text**.

### Submit Article Link

<img src="images/ui2.png" height="2000" width="4000">

* Automatically extracts article content
* Accepts optional publisher input

### Submit Article Text

<img src="images/ui3.png" height="2000" width="4000">

* Allows direct text submission
* Accepts optional publisher input

---

## ğŸ“Š Example Output

### Credibility Result Page

<img src="images/output2.png" height="2000" width="4000">

The result view presents:

* Final credibility score (out of 30)
* Verdict interpretation
* Content category classification
* Publisher credibility score
* Contribution of external validation

---

## ğŸ¤– Model Training

The project includes a complete **BERT fine-tuning pipeline** using HuggingFace Transformers.

* Model: `bert-base-uncased`
* Labels: Neutral, Biased, Contradictory
* Frameworks: PyTorch, HuggingFace Trainer

### Training Progress

<img src="images/during training.png" height="1000" width="3000">

### Evaluation Results

<img src="images/eval test.png" height="500" width="2000">

The model demonstrates strong contextual understanding and high accuracy on the held-out test set.

---

## ğŸ—‚ï¸ Project Structure

```text
fake_news_detector/
â”‚
â”œâ”€â”€ app.py                  # Flask app entry point
â”œâ”€â”€ config.py               # Environment & configuration loading
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ scraping/               # Article extraction logic
â”‚   â”œâ”€â”€ article_reader.py
â”‚   â””â”€â”€ scraper.py
â”‚
â”œâ”€â”€ model/                  # NLP models
â”‚   â”œâ”€â”€ bert_classifier.py
â”‚   â””â”€â”€ train_bert.py
â”‚
â”œâ”€â”€ scoring/                # Scoring & verdict logic
â”‚   â”œâ”€â”€ category_score.py
â”‚   â”œâ”€â”€ publisher_score.py
â”‚   â””â”€â”€ verdict.py
â”‚
â”œâ”€â”€ database/               # Supabase integration
â”‚   â”œâ”€â”€ supabase_client.py
â”‚   â””â”€â”€ schema.sql
â”‚
â”œâ”€â”€ utils/                  # Helper utilities
â”‚   â”œâ”€â”€ text_cleaner.py
â”‚   â””â”€â”€ publisher_normalizer.py
|   â”œâ”€â”€ external_validation.py
â”‚
â”œâ”€â”€ data/                   # Training datasets
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ val.csv
â”‚   â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ templates/              # HTML UI templates
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ article_link.html
â”‚   â””â”€â”€ article_text.html
â”‚
â””â”€â”€ .env
```

---

## âš™ï¸ Tech Stack

* **Backend**: Python 3.10, Flask
* **NLP / ML**: BERT, HuggingFace Transformers, PyTorch
* **External Validation**: Web search & similarity matching
* **Scraping**: Firecrawl, Requests, BeautifulSoup
* **Database**: Supabase (PostgreSQL)
* **Frontend**: HTML, CSS (Flask templates)

---

## ğŸš€ Running the Project

```bash
pip install -r requirements.txt
python app.py
```

Ensure your `.env` file contains:

```env
FIRECRAWL_API_KEY=your_key
SUPABASE_URL=your_url
SUPABASE_KEY=your_key
```

---

## âš ï¸ Disclaimer

This system provides an **automated credibility assessment** intended as a **decision-support tool**. It does not replace professional fact-checking or journalistic verification.

---

## ğŸ‘¨â€ğŸ’» Author

Built as part of an AI/ML-focused project exploring **misinformation detection**, **multi-source validation**, **explainable scoring**, and **responsible NLP deployment**.

If you find this project useful, consider â­ starring the repository and experimenting with further enhancements.
